# Global
seed: 531997
device: cpu

# agent
agent:
  temperature: 0.1
  observe_lambda: False

# environment
training_env:
  env_name: "SH1_NA"
  lambda_scale: 0.95
  terminal_backlog: 200
  inverse_reward: True
  env_generator_seed: 531997

eval_envs:
  lambda_scales: 0.95
  env_generator_seed: 531997
  inverse_reward: True
  terminal_backlog: 200

# collector per env
collector:
  total_training_frames: 5000000
  frames_per_batch: 5000
  frame_skip: 1

# eval
eval:
  eval_interval: 15000
  num_eval_envs: 3
  traj_steps: 50000


# logger
logger:
  backend: wandb
  project_name: experiment4
  group_name: null
  run_name: ["scaled_lambda", "PPO"]


# Optim
optim:
  lr:  1.0e-4
  eps: 1.0e-6
  weight_decay: 0.0
  max_grad_norm: 1.0
  anneal_lr: True
  clip_epsilon: False
  alpha: 0.99

# loss
loss:
  gamma: 0.99
  mini_batch_size: 100
  ppo_epochs: 3
  gae_lambda: 0.95
  clip_epsilon: 0.1
  anneal_clip_epsilon: False
  critic_coef: 1.0
  entropy_coef: 0.00
  loss_critic_type: l2
  norm_advantage: False

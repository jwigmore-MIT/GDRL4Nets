# Global
seed: 531997
device: cpu

# agent
agent:
  temperature: 5
  observe_lambda: False
  init_weight: 0.25

# environment
training_env:
  env_name: "SH1"
  terminal_backlog: 1000
  inverse_reward: True
  env_generator_seed: 531997

eval_envs:
  env_generator_seed: 531997
  inverse_reward: True
  terminal_backlog: 500

# collector per env
collector:
  total_training_frames: 3000000
  frames_per_batch: 500
  frame_skip: 1
  reset_at_each_iter: False

# eval
eval:
  eval_interval: 100000
  num_eval_envs: 3
  traj_steps: 10000
  evaluate_before_training: False



# logger
logger:
  backend: wandb
  project_name: experiment14
  group_name: null
  run_name: ["TrainGen", "PPO"]


# Optim
optim:
  lr:  1.0e-2
  eps: 1.0e-6
  weight_decay: 0.00
  max_grad_norm: 1.0
  anneal_lr: False
  clip_epsilon: False
  alpha: 0.99

# loss
loss:
  gamma: 0.99
  mini_batch_size: 100
  ppo_epochs: 3
  gae_lambda: 0.95
  clip_epsilon: 0.1
  anneal_clip_epsilon: False
  critic_coef: 0.5
  entropy_coef: 0.01
  loss_critic_type: l2
  norm_advantage: True

# Global
seed: 531997
device: cpu

# agent
agent:
  temperature: 0.1
  observe_lambda: True

# environment
training_env:
  env_name: "SH2u"
  terminal_backlog: 500
  inverse_reward: True
  env_generator_seed: 531997

eval_envs:
  env_generator_seed: 531997
  inverse_reward: True
  terminal_backlog: 500

# collector per env
collector:
  total_training_frames: 3000000
  frames_per_batch: 5000
  frame_skip: 1

# eval
eval:
  eval_interval: 50000
  num_eval_envs: 3
  traj_steps: 50000


# logger
logger:
  backend: wandb
  project_name: experiment8
  group_name: null
  run_name: ["TrainGen", "PPO"]


# Optim
optim:
  lr:  5.0e-4
  eps: 1.0e-6
  weight_decay: 0.0
  max_grad_norm: 1.0
  anneal_lr: True
  clip_epsilon: False
  alpha: 0.99

# loss
loss:
  gamma: 0.99
  mini_batch_size: 100
  ppo_epochs: 3
  gae_lambda: 0.95
  clip_epsilon: 0.1
  anneal_clip_epsilon: False
  critic_coef: 0.5
  entropy_coef: 0.01
  loss_critic_type: l2
  norm_advantage: True

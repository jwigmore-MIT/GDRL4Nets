# Global
seed: 531997
device: cpu

# agent
agent:
  temperature: 0.1
  observe_lambda: False
  actor_depth: 2
  actor_hidden_size: 64

# environment
training_env:
  env_name: "SH1"
  terminal_backlog: 1000
  inverse_reward: True
  env_generator_seed: 531997

eval_envs:
  env_generator_seed: 531997
  inverse_reward: True
  terminal_backlog: 500

# collector per env
collector:
  total_training_frames: 3000000
  frames_per_batch: 5000
  frame_skip: 1
  reset_at_each_iter: False

# eval
eval:
  eval_interval: 100000
  num_eval_envs: 3
  traj_steps: 50000


# logger
logger:
  backend: wandb
  project_name: experiment11
  group_name: null
  run_name: ["TrainGen", "PPO"]


# Optim
optim:
  lr:  3.0e-4
  eps: 1.0e-6
  weight_decay: 0.00
  max_grad_norm: 1.0
  anneal_lr: True
  clip_epsilon: False
  alpha: 0.99

# loss
loss:
  gamma: 0.99
  mini_batch_size: 100
  ppo_epochs: 3
  gae_lambda: 0.95
  clip_epsilon: 0.1
  anneal_clip_epsilon: False
  critic_coef: 0.5
  entropy_coef: 0.01
  loss_critic_type: l2
  norm_advantage: True
